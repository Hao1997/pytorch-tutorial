{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from trainer import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8257,  0.5835,  0.8889],\n",
       "        [ 0.2154,  0.2320,  0.0178],\n",
       "        [-1.7530, -0.0785, -0.2000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# populate a 3x3 tensor with samples drawn from a random distribution\n",
    "x = torch.randn(3, 3)\n",
    "y = torch.randn(3, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2445, -0.1320,  0.4195],\n",
       "        [ 1.1207, -1.2937,  0.0059],\n",
       "        [-1.4143, -0.5531, -0.3274]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.8257, 0.5835, 0.8889])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can index into this guy just like a python list!\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also do some boolean stuff with him\n",
    "x > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: tensor([[ 1.8257,  0.5835,  0.8889,  0.2445, -0.1320,  0.4195],\n",
      "        [ 0.2154,  0.2320,  0.0178,  1.1207, -1.2937,  0.0059],\n",
      "        [-1.7530, -0.0785, -0.2000, -1.4143, -0.5531, -0.3274]])\n",
      "z.size() torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# can also concatenate two tensors together on various dimensions\n",
    "z = torch.cat((x, y), 1)\n",
    "print(\"z:\", z)\n",
    "print(\"z.size()\", z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8257,  0.5835],\n",
       "        [ 0.8889,  0.2445],\n",
       "        [-0.1320,  0.4195],\n",
       "        [ 0.2154,  0.2320],\n",
       "        [ 0.0178,  1.1207],\n",
       "        [-1.2937,  0.0059],\n",
       "        [-1.7530, -0.0785],\n",
       "        [-0.2000, -1.4143],\n",
       "        [-0.5531, -0.3274]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and reshape them\n",
    "z.view(9, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8257,  0.5835],\n",
       "         [ 0.8889,  0.2445],\n",
       "         [-0.1320,  0.4195]],\n",
       "\n",
       "        [[ 0.2154,  0.2320],\n",
       "         [ 0.0178,  1.1207],\n",
       "         [-1.2937,  0.0059]],\n",
       "\n",
       "        [[-1.7530, -0.0785],\n",
       "         [-0.2000, -1.4143],\n",
       "         [-0.5531, -0.3274]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in different ways\n",
    "z.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0702,  0.4515,  1.3083],\n",
       "        [ 1.3360, -1.0617,  0.0237],\n",
       "        [-3.1673, -0.6317, -0.5274]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can add elementwise\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1569, -1.4874,  0.4783],\n",
       "        [ 0.2875, -0.3384,  0.0859],\n",
       "        [-0.2338,  0.4436, -0.6703]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can also matrix multiply! (this is important)\n",
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of matrix a: torch.Size([4, 3])\n",
      "size of matrix b: torch.Size([4, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [4 x 3], m2: [4 x 3] at /Users/soumith/miniconda2/conda-bld/pytorch_1532624435833/work/aten/src/TH/generic/THTensorMath.cpp:2070",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c18c2e9fec0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size of matrix a:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"size of matrix b:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# literally the bane of my existence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [4 x 3], m2: [4 x 3] at /Users/soumith/miniconda2/conda-bld/pytorch_1532624435833/work/aten/src/TH/generic/THTensorMath.cpp:2070"
     ]
    }
   ],
   "source": [
    "# most important lesson from linear algebra: if you want to multiply some matrix `a` with some matrix `b`\n",
    "# `a` needs to be of size (m x n) and `b` needs to be of size (n x p)\n",
    "# this will give you a final matrix c of size (m x p)\n",
    "# here is the error message you will get if that's not the case\n",
    "a = torch.randn((4, 3))\n",
    "b = torch.randn((4, 3))\n",
    "print(\"size of matrix a:\", a.size())\n",
    "print(\"size of matrix b:\", b.size())\n",
    "torch.matmul(a, b)\n",
    "\n",
    "# literally the bane of my existence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solutions! \n",
    "torch.matmul(a, torch.transpose(b, dim0=0, dim1=1))  # be careful though if you're working with batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers (Low-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor([[-1.5729,  0.8726, -0.8991],\n",
      "        [-0.0950,  0.0925,  1.8261],\n",
      "        [-0.1823,  0.8657,  0.6824]])\n",
      "x.size(): torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# define a new matrix x\n",
    "x = torch.randn(3, 3)\n",
    "print(\"x: \", x)\n",
    "print(\"x.size():\", x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a weight matrix \n",
    "w1 = torch.randn(6, 3)\n",
    "b1 = torch.randn(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4739,  0.8955, -0.0811],\n",
       "        [ 0.6793, -0.7189,  1.6245],\n",
       "        [ 0.4240, -0.0701,  0.0375],\n",
       "        [-0.4710, -0.7020,  0.3119],\n",
       "        [-1.8891, -1.5711,  1.2038],\n",
       "        [-1.0248,  1.8811,  0.7875]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4235,  2.0292,  0.0034, -1.7639, -0.5050, -1.3253])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1089, -3.1563, -0.7617, -0.1521,  0.5181,  2.5452],\n",
       "        [-0.1103,  2.8354,  0.0216,  0.5493,  2.2323,  1.7094],\n",
       "        [ 0.6335,  0.3625, -0.1124, -0.3090, -0.1943,  2.3526]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiply input by weight matrix\n",
    "layer1_output = torch.matmul(x, torch.transpose(w1, 0, 1))\n",
    "layer1_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3146, -1.1271, -0.7583, -1.9160,  0.0130,  1.2198],\n",
       "        [-0.5338,  4.8646,  0.0250, -1.2146,  1.7272,  0.3841],\n",
       "        [ 0.2099,  2.3917, -0.1090, -2.0729, -0.6993,  1.0273]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the bias term \n",
    "layer1_output = layer1_output + b1\n",
    "layer1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers (High-Level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer1 = nn.Linear(in_features=3, out_features=6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9879,  0.0411,  0.8826, -0.5743,  0.5697,  0.6398],\n",
       "        [ 0.6288, -1.0389, -0.7006,  0.4881,  1.3127,  1.4414],\n",
       "        [ 0.7568, -0.6123, -0.5626,  0.1392,  0.6643,  1.2159]],\n",
       "       grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = linear_layer1(x)\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.3070,  0.3160,  0.1251],\n",
       "        [-0.1028,  0.0630, -0.3225],\n",
       "        [-0.5709, -0.4983, -0.4140],\n",
       "        [ 0.1493,  0.0390,  0.3200],\n",
       "        [-0.5555, -0.0908,  0.5479],\n",
       "        [-0.2313,  0.5253,  0.5699]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.3419, -0.4655,  0.0472, -0.0857,  0.2678,  0.3301], requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer1.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = MNIST(root=\".\", download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the size of the first sample in mnist\n",
    "mnist[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        define model attributes\n",
    "        \"\"\"\n",
    "        self.layer1 = nn.Linear(28*28, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        define model\n",
    "        \"\"\"\n",
    "        x = x.view(-1, 28*28)\n",
    "        h1 = self.relu(self.layer1(x))\n",
    "        h2 = self.relu(self.layer2(h1))\n",
    "        logits = self.relu(self.layer3(h2))\n",
    "        \n",
    "        return logits  # usually return `logits` (output of final layer) or `probs` (softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Run This Guy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model and trainer\n",
    "model = Model()\n",
    "trainer = Trainer(model=model, save_dir=\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1875 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/1875 [00:00<00:49, 38.14it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 8/1875 [00:00<00:50, 37.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.859:   0%|          | 8/1875 [00:00<00:50, 37.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.859:   1%|          | 12/1875 [00:00<00:50, 36.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.859:   1%|          | 17/1875 [00:00<00:46, 39.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.786:   1%|          | 17/1875 [00:00<00:46, 39.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.786:   1%|          | 22/1875 [00:00<00:46, 40.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.786:   1%|▏         | 27/1875 [00:00<00:43, 42.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   1%|▏         | 27/1875 [00:00<00:43, 42.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   2%|▏         | 32/1875 [00:00<00:43, 42.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   2%|▏         | 38/1875 [00:00<00:41, 44.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.842:   2%|▏         | 38/1875 [00:00<00:41, 44.72it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.842:   2%|▏         | 43/1875 [00:01<00:40, 45.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.842:   3%|▎         | 49/1875 [00:01<00:38, 47.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.828:   3%|▎         | 49/1875 [00:01<00:38, 47.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.828:   3%|▎         | 54/1875 [00:01<00:41, 44.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.828:   3%|▎         | 59/1875 [00:01<00:40, 45.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:   3%|▎         | 59/1875 [00:01<00:40, 45.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:   3%|▎         | 64/1875 [00:01<00:40, 44.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:   4%|▎         | 69/1875 [00:01<00:39, 45.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.008:   4%|▎         | 69/1875 [00:01<00:39, 45.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.008:   4%|▍         | 74/1875 [00:01<00:40, 44.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.008:   4%|▍         | 79/1875 [00:01<00:39, 45.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.020:   4%|▍         | 79/1875 [00:01<00:39, 45.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.020:   4%|▍         | 84/1875 [00:01<00:43, 41.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.020:   5%|▍         | 89/1875 [00:02<00:45, 39.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   5%|▍         | 89/1875 [00:02<00:45, 39.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   5%|▌         | 94/1875 [00:02<00:45, 39.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   5%|▌         | 99/1875 [00:02<00:43, 40.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.894:   5%|▌         | 99/1875 [00:02<00:43, 40.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.894:   6%|▌         | 104/1875 [00:02<00:47, 37.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.894:   6%|▌         | 109/1875 [00:02<00:44, 39.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.998:   6%|▌         | 109/1875 [00:02<00:44, 39.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.998:   6%|▌         | 114/1875 [00:02<00:45, 38.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.998:   6%|▋         | 118/1875 [00:02<00:52, 33.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   6%|▋         | 118/1875 [00:02<00:52, 33.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   7%|▋         | 122/1875 [00:03<00:53, 32.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.988:   7%|▋         | 126/1875 [00:03<00:56, 30.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.044:   7%|▋         | 126/1875 [00:03<00:56, 30.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.044:   7%|▋         | 131/1875 [00:03<00:53, 32.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.044:   7%|▋         | 135/1875 [00:03<00:51, 33.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.924:   7%|▋         | 135/1875 [00:03<00:51, 33.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.924:   7%|▋         | 140/1875 [00:03<00:48, 35.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.924:   8%|▊         | 144/1875 [00:03<00:46, 36.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.924:   8%|▊         | 149/1875 [00:03<00:43, 39.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   8%|▊         | 149/1875 [00:03<00:43, 39.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   8%|▊         | 154/1875 [00:03<00:42, 40.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:   8%|▊         | 159/1875 [00:04<00:46, 37.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.963:   8%|▊         | 159/1875 [00:04<00:46, 37.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.963:   9%|▊         | 163/1875 [00:04<00:47, 36.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.963:   9%|▉         | 168/1875 [00:04<00:44, 38.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.900:   9%|▉         | 168/1875 [00:04<00:44, 38.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.900:   9%|▉         | 172/1875 [00:04<00:44, 38.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.900:   9%|▉         | 177/1875 [00:04<00:41, 40.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.922:   9%|▉         | 177/1875 [00:04<00:41, 40.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.922:  10%|▉         | 182/1875 [00:04<00:40, 41.88it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.922:  10%|▉         | 187/1875 [00:04<00:41, 40.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.956:  10%|▉         | 187/1875 [00:04<00:41, 40.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.956:  10%|█         | 192/1875 [00:04<00:44, 37.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.956:  10%|█         | 196/1875 [00:04<00:45, 36.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.055:  10%|█         | 196/1875 [00:05<00:45, 36.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.055:  11%|█         | 200/1875 [00:05<00:44, 37.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.055:  11%|█         | 205/1875 [00:05<00:42, 39.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.968:  11%|█         | 205/1875 [00:05<00:42, 39.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.968:  11%|█         | 210/1875 [00:05<00:40, 40.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.968:  11%|█▏        | 215/1875 [00:05<00:38, 42.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.977:  11%|█▏        | 215/1875 [00:05<00:38, 42.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.977:  12%|█▏        | 220/1875 [00:05<00:38, 43.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.977:  12%|█▏        | 225/1875 [00:05<00:37, 43.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  12%|█▏        | 225/1875 [00:05<00:37, 43.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  12%|█▏        | 230/1875 [00:05<00:39, 41.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  13%|█▎        | 235/1875 [00:05<00:40, 40.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  13%|█▎        | 235/1875 [00:05<00:40, 40.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  13%|█▎        | 240/1875 [00:05<00:40, 40.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  13%|█▎        | 245/1875 [00:06<00:39, 41.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.850:  13%|█▎        | 245/1875 [00:06<00:39, 41.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.850:  13%|█▎        | 250/1875 [00:06<00:39, 41.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.850:  14%|█▎        | 255/1875 [00:06<00:38, 41.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.054:  14%|█▎        | 255/1875 [00:06<00:38, 41.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.054:  14%|█▍        | 260/1875 [00:06<00:37, 42.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.054:  14%|█▍        | 265/1875 [00:06<00:37, 42.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.940:  14%|█▍        | 265/1875 [00:06<00:37, 42.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.940:  14%|█▍        | 270/1875 [00:06<00:37, 43.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.940:  15%|█▍        | 275/1875 [00:06<00:36, 43.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.955:  15%|█▍        | 275/1875 [00:06<00:36, 43.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.955:  15%|█▍        | 280/1875 [00:06<00:36, 43.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.955:  15%|█▌        | 285/1875 [00:07<00:35, 44.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  15%|█▌        | 285/1875 [00:07<00:35, 44.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  15%|█▌        | 290/1875 [00:07<00:34, 45.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  16%|█▌        | 295/1875 [00:07<00:37, 42.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  16%|█▌        | 295/1875 [00:07<00:37, 42.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  16%|█▌        | 300/1875 [00:07<00:35, 43.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.942:  16%|█▋        | 305/1875 [00:07<00:34, 45.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  16%|█▋        | 305/1875 [00:07<00:34, 45.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  17%|█▋        | 310/1875 [00:07<00:34, 45.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  17%|█▋        | 315/1875 [00:07<00:33, 46.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  17%|█▋        | 315/1875 [00:07<00:33, 46.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  17%|█▋        | 320/1875 [00:07<00:34, 44.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.012:  17%|█▋        | 325/1875 [00:07<00:36, 42.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  17%|█▋        | 325/1875 [00:08<00:36, 42.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  18%|█▊        | 330/1875 [00:08<00:35, 43.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.071:  18%|█▊        | 335/1875 [00:08<00:33, 45.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:  18%|█▊        | 335/1875 [00:08<00:33, 45.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.952:  18%|█▊        | 340/1875 [00:08<00:33, 45.73it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.952:  18%|█▊        | 345/1875 [00:08<00:33, 45.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.999:  18%|█▊        | 345/1875 [00:08<00:33, 45.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.999:  19%|█▊        | 350/1875 [00:08<00:35, 43.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.999:  19%|█▉        | 355/1875 [00:08<00:33, 45.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.929:  19%|█▉        | 355/1875 [00:08<00:33, 45.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.929:  19%|█▉        | 360/1875 [00:08<00:33, 44.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.929:  19%|█▉        | 365/1875 [00:08<00:34, 43.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  19%|█▉        | 365/1875 [00:08<00:34, 43.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  20%|█▉        | 370/1875 [00:08<00:34, 43.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  20%|██        | 375/1875 [00:09<00:33, 45.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.991:  20%|██        | 375/1875 [00:09<00:33, 45.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.991:  20%|██        | 380/1875 [00:09<00:33, 45.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.991:  21%|██        | 385/1875 [00:09<00:33, 44.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.932:  21%|██        | 385/1875 [00:09<00:33, 44.21it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.932:  21%|██        | 390/1875 [00:09<00:33, 44.95it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.932:  21%|██        | 395/1875 [00:09<00:32, 45.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.035:  21%|██        | 395/1875 [00:09<00:32, 45.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.035:  21%|██▏       | 400/1875 [00:09<00:31, 46.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.035:  22%|██▏       | 405/1875 [00:09<00:32, 44.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.916:  22%|██▏       | 405/1875 [00:09<00:32, 44.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.916:  22%|██▏       | 410/1875 [00:09<00:32, 44.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.916:  22%|██▏       | 415/1875 [00:09<00:32, 45.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  22%|██▏       | 415/1875 [00:10<00:32, 45.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  22%|██▏       | 420/1875 [00:10<00:31, 45.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.994:  23%|██▎       | 425/1875 [00:10<00:31, 46.37it/s]\u001b[A\u001b[A\n",
      "Training Loss: 0.899:  76%|███████▌  | 1424/1875 [01:03<00:16, 27.16it/s]\u001b[A\n",
      "\n",
      "Training Loss: 0.992:  23%|██▎       | 425/1875 [00:10<00:31, 46.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.992:  23%|██▎       | 430/1875 [00:10<00:32, 44.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.992:  23%|██▎       | 435/1875 [00:10<00:32, 44.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.976:  23%|██▎       | 435/1875 [00:10<00:32, 44.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.976:  23%|██▎       | 440/1875 [00:10<00:33, 42.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.976:  24%|██▎       | 445/1875 [00:10<00:39, 36.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.050:  24%|██▎       | 445/1875 [00:10<00:39, 36.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.050:  24%|██▍       | 450/1875 [00:10<00:37, 38.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.050:  24%|██▍       | 454/1875 [00:10<00:36, 38.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.050:  24%|██▍       | 459/1875 [00:11<00:35, 39.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.926:  24%|██▍       | 459/1875 [00:11<00:35, 39.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.926:  25%|██▍       | 464/1875 [00:11<00:34, 41.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.926:  25%|██▌       | 469/1875 [00:11<00:34, 40.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.985:  25%|██▌       | 469/1875 [00:11<00:34, 40.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.985:  25%|██▌       | 474/1875 [00:11<00:33, 41.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.985:  26%|██▌       | 479/1875 [00:11<00:32, 42.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.019:  26%|██▌       | 479/1875 [00:11<00:32, 42.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.019:  26%|██▌       | 484/1875 [00:11<00:34, 40.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 1.019:  26%|██▌       | 489/1875 [00:11<00:32, 42.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.967:  26%|██▌       | 489/1875 [00:11<00:32, 42.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.967:  26%|██▋       | 494/1875 [00:11<00:31, 43.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.967:  27%|██▋       | 499/1875 [00:11<00:31, 43.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.981:  27%|██▋       | 499/1875 [00:11<00:31, 43.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.981:  27%|██▋       | 504/1875 [00:12<00:33, 41.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.981:  27%|██▋       | 509/1875 [00:12<00:34, 39.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.752:  27%|██▋       | 509/1875 [00:12<00:34, 39.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.752:  27%|██▋       | 513/1875 [00:12<00:35, 38.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.752:  28%|██▊       | 518/1875 [00:12<00:33, 40.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  28%|██▊       | 518/1875 [00:12<00:33, 40.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  28%|██▊       | 523/1875 [00:12<00:33, 40.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  28%|██▊       | 528/1875 [00:12<00:31, 42.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  28%|██▊       | 528/1875 [00:12<00:31, 42.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  28%|██▊       | 533/1875 [00:12<00:33, 40.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.878:  29%|██▊       | 538/1875 [00:12<00:31, 41.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  29%|██▊       | 538/1875 [00:12<00:31, 41.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  29%|██▉       | 543/1875 [00:13<00:32, 41.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  29%|██▉       | 548/1875 [00:13<00:31, 41.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.718:  29%|██▉       | 548/1875 [00:13<00:31, 41.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.718:  29%|██▉       | 553/1875 [00:13<00:31, 41.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.718:  30%|██▉       | 558/1875 [00:13<00:31, 42.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.663:  30%|██▉       | 558/1875 [00:13<00:31, 42.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.663:  30%|███       | 563/1875 [00:13<00:32, 39.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.663:  30%|███       | 569/1875 [00:13<00:30, 42.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.678:  30%|███       | 569/1875 [00:13<00:30, 42.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.678:  31%|███       | 574/1875 [00:13<00:29, 44.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.678:  31%|███       | 579/1875 [00:13<00:32, 40.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.638:  31%|███       | 579/1875 [00:13<00:32, 40.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.638:  31%|███       | 584/1875 [00:14<00:31, 41.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.736:  31%|███       | 584/1875 [00:14<00:31, 41.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.736:  31%|███▏      | 590/1875 [00:14<00:29, 44.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.736:  32%|███▏      | 595/1875 [00:14<00:28, 45.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  32%|███▏      | 595/1875 [00:14<00:28, 45.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  32%|███▏      | 600/1875 [00:14<00:32, 39.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  32%|███▏      | 605/1875 [00:14<00:31, 40.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  32%|███▏      | 605/1875 [00:14<00:31, 40.27it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  33%|███▎      | 610/1875 [00:14<00:29, 42.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  33%|███▎      | 615/1875 [00:14<00:29, 42.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.892:  33%|███▎      | 615/1875 [00:14<00:29, 42.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.892:  33%|███▎      | 620/1875 [00:14<00:31, 39.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.892:  33%|███▎      | 625/1875 [00:15<00:31, 39.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.755:  33%|███▎      | 625/1875 [00:15<00:31, 39.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.755:  34%|███▎      | 630/1875 [00:15<00:30, 40.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.755:  34%|███▍      | 635/1875 [00:15<00:29, 42.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.684:  34%|███▍      | 635/1875 [00:15<00:29, 42.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.684:  34%|███▍      | 640/1875 [00:15<00:28, 43.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.684:  34%|███▍      | 645/1875 [00:15<00:27, 44.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  34%|███▍      | 645/1875 [00:15<00:27, 44.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  35%|███▍      | 651/1875 [00:15<00:26, 46.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  35%|███▌      | 657/1875 [00:15<00:25, 48.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.749:  35%|███▌      | 657/1875 [00:15<00:25, 48.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.749:  35%|███▌      | 662/1875 [00:15<00:24, 48.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.749:  36%|███▌      | 668/1875 [00:15<00:24, 49.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.823:  36%|███▌      | 668/1875 [00:15<00:24, 49.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.823:  36%|███▌      | 673/1875 [00:16<00:25, 47.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.823:  36%|███▌      | 679/1875 [00:16<00:24, 48.28it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.737:  36%|███▌      | 679/1875 [00:16<00:24, 48.28it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.737:  37%|███▋      | 685/1875 [00:16<00:24, 48.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.656:  37%|███▋      | 685/1875 [00:16<00:24, 48.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.656:  37%|███▋      | 690/1875 [00:16<00:24, 48.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.656:  37%|███▋      | 695/1875 [00:16<00:24, 48.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.805:  37%|███▋      | 695/1875 [00:16<00:24, 48.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.805:  37%|███▋      | 700/1875 [00:16<00:24, 48.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.805:  38%|███▊      | 705/1875 [00:16<00:24, 48.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.795:  38%|███▊      | 705/1875 [00:16<00:24, 48.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.795:  38%|███▊      | 710/1875 [00:16<00:24, 47.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.795:  38%|███▊      | 715/1875 [00:16<00:24, 47.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.867:  38%|███▊      | 715/1875 [00:16<00:24, 47.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.867:  38%|███▊      | 720/1875 [00:16<00:24, 46.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.867:  39%|███▊      | 726/1875 [00:17<00:24, 47.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.824:  39%|███▊      | 726/1875 [00:17<00:24, 47.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.824:  39%|███▉      | 731/1875 [00:17<00:23, 47.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.824:  39%|███▉      | 736/1875 [00:17<00:23, 48.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.883:  39%|███▉      | 736/1875 [00:17<00:23, 48.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.883:  40%|███▉      | 741/1875 [00:17<00:23, 48.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.883:  40%|███▉      | 746/1875 [00:17<00:23, 48.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.621:  40%|███▉      | 746/1875 [00:17<00:23, 48.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.621:  40%|████      | 751/1875 [00:17<00:23, 47.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.621:  40%|████      | 756/1875 [00:17<00:23, 47.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  40%|████      | 756/1875 [00:17<00:23, 47.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  41%|████      | 761/1875 [00:17<00:24, 46.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.708:  41%|████      | 766/1875 [00:17<00:25, 43.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.633:  41%|████      | 766/1875 [00:18<00:25, 43.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.633:  41%|████      | 771/1875 [00:18<00:25, 42.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.633:  41%|████▏     | 776/1875 [00:18<00:25, 42.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.845:  41%|████▏     | 776/1875 [00:18<00:25, 42.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.845:  42%|████▏     | 781/1875 [00:18<00:27, 39.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.845:  42%|████▏     | 786/1875 [00:18<00:28, 38.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  42%|████▏     | 786/1875 [00:18<00:28, 38.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  42%|████▏     | 790/1875 [00:18<00:28, 38.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  42%|████▏     | 794/1875 [00:18<00:29, 37.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  43%|████▎     | 798/1875 [00:18<00:30, 35.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.762:  43%|████▎     | 798/1875 [00:18<00:30, 35.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.762:  43%|████▎     | 802/1875 [00:18<00:31, 33.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.762:  43%|████▎     | 806/1875 [00:19<00:31, 33.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  43%|████▎     | 806/1875 [00:19<00:31, 33.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  43%|████▎     | 810/1875 [00:19<00:31, 34.06it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  43%|████▎     | 814/1875 [00:19<00:30, 34.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.818:  44%|████▎     | 819/1875 [00:19<00:28, 37.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  44%|████▎     | 819/1875 [00:19<00:28, 37.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  44%|████▍     | 824/1875 [00:19<00:26, 39.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  44%|████▍     | 824/1875 [00:19<00:26, 39.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  44%|████▍     | 830/1875 [00:19<00:25, 41.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.655:  45%|████▍     | 835/1875 [00:19<00:23, 43.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.800:  45%|████▍     | 835/1875 [00:19<00:23, 43.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.800:  45%|████▍     | 840/1875 [00:19<00:22, 45.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.800:  45%|████▌     | 845/1875 [00:19<00:23, 43.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  45%|████▌     | 845/1875 [00:20<00:23, 43.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  45%|████▌     | 850/1875 [00:20<00:24, 41.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  46%|████▌     | 855/1875 [00:20<00:25, 39.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.596:  46%|████▌     | 855/1875 [00:20<00:25, 39.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.596:  46%|████▌     | 860/1875 [00:20<00:24, 41.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.596:  46%|████▌     | 865/1875 [00:20<00:23, 42.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.732:  46%|████▌     | 865/1875 [00:20<00:23, 42.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.732:  46%|████▋     | 870/1875 [00:20<00:23, 43.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.732:  47%|████▋     | 875/1875 [00:20<00:22, 45.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  47%|████▋     | 875/1875 [00:20<00:22, 45.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  47%|████▋     | 880/1875 [00:20<00:23, 42.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  47%|████▋     | 885/1875 [00:20<00:25, 38.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.681:  47%|████▋     | 889/1875 [00:21<00:26, 37.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.622:  47%|████▋     | 889/1875 [00:21<00:26, 37.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.622:  48%|████▊     | 893/1875 [00:21<00:27, 35.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.622:  48%|████▊     | 897/1875 [00:21<00:27, 35.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  48%|████▊     | 897/1875 [00:21<00:27, 35.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  48%|████▊     | 901/1875 [00:21<00:27, 35.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.809:  48%|████▊     | 905/1875 [00:21<00:26, 36.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  48%|████▊     | 905/1875 [00:21<00:26, 36.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  49%|████▊     | 910/1875 [00:21<00:26, 37.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  49%|████▊     | 914/1875 [00:21<00:25, 37.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  49%|████▉     | 919/1875 [00:21<00:24, 39.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.803:  49%|████▉     | 919/1875 [00:21<00:24, 39.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.803:  49%|████▉     | 924/1875 [00:22<00:23, 39.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.803:  50%|████▉     | 929/1875 [00:22<00:23, 40.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.647:  50%|████▉     | 929/1875 [00:22<00:23, 40.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.647:  50%|████▉     | 934/1875 [00:22<00:25, 37.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.647:  50%|█████     | 938/1875 [00:22<00:34, 27.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  50%|█████     | 938/1875 [00:22<00:34, 27.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  50%|█████     | 942/1875 [00:22<00:33, 27.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.776:  50%|█████     | 946/1875 [00:22<00:32, 28.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.744:  50%|█████     | 946/1875 [00:23<00:32, 28.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.744:  51%|█████     | 950/1875 [00:23<00:40, 23.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.744:  51%|█████     | 953/1875 [00:23<00:39, 23.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.744:  51%|█████     | 957/1875 [00:23<00:35, 25.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.781:  51%|█████     | 957/1875 [00:23<00:35, 25.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.781:  51%|█████▏    | 961/1875 [00:23<00:33, 27.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.781:  51%|█████▏    | 964/1875 [00:23<00:34, 26.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.781:  52%|█████▏    | 968/1875 [00:23<00:32, 27.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.834:  52%|█████▏    | 968/1875 [00:23<00:32, 27.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.834:  52%|█████▏    | 972/1875 [00:23<00:31, 28.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.834:  52%|█████▏    | 975/1875 [00:23<00:32, 28.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.834:  52%|█████▏    | 979/1875 [00:24<00:29, 30.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.748:  52%|█████▏    | 979/1875 [00:24<00:29, 30.14it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.748:  52%|█████▏    | 983/1875 [00:24<00:28, 30.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.748:  53%|█████▎    | 987/1875 [00:24<00:27, 32.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.639:  53%|█████▎    | 987/1875 [00:24<00:27, 32.19it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.639:  53%|█████▎    | 991/1875 [00:24<00:26, 33.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.639:  53%|█████▎    | 996/1875 [00:24<00:24, 35.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.711:  53%|█████▎    | 996/1875 [00:24<00:24, 35.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.711:  53%|█████▎    | 1000/1875 [00:24<00:23, 36.68it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.711:  54%|█████▎    | 1004/1875 [00:24<00:23, 37.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.711:  54%|█████▍    | 1009/1875 [00:24<00:22, 38.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.796:  54%|█████▍    | 1009/1875 [00:24<00:22, 38.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.796:  54%|█████▍    | 1013/1875 [00:24<00:22, 38.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.796:  54%|█████▍    | 1018/1875 [00:25<00:21, 39.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  54%|█████▍    | 1018/1875 [00:25<00:21, 39.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  55%|█████▍    | 1022/1875 [00:25<00:25, 33.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  55%|█████▍    | 1027/1875 [00:25<00:23, 36.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.793:  55%|█████▍    | 1027/1875 [00:25<00:23, 36.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.793:  55%|█████▍    | 1031/1875 [00:25<00:22, 36.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.793:  55%|█████▌    | 1035/1875 [00:25<00:22, 37.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.734:  55%|█████▌    | 1035/1875 [00:25<00:22, 37.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.734:  55%|█████▌    | 1040/1875 [00:25<00:21, 38.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.734:  56%|█████▌    | 1045/1875 [00:25<00:20, 40.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.701:  56%|█████▌    | 1045/1875 [00:25<00:20, 40.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.701:  56%|█████▌    | 1050/1875 [00:25<00:20, 40.35it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.701:  56%|█████▋    | 1055/1875 [00:25<00:20, 40.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  56%|█████▋    | 1055/1875 [00:26<00:20, 40.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  57%|█████▋    | 1060/1875 [00:26<00:19, 40.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  57%|█████▋    | 1065/1875 [00:26<00:21, 37.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.703:  57%|█████▋    | 1069/1875 [00:26<00:21, 36.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  57%|█████▋    | 1069/1875 [00:26<00:21, 36.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  57%|█████▋    | 1073/1875 [00:26<00:21, 37.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  57%|█████▋    | 1078/1875 [00:26<00:20, 38.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.581:  57%|█████▋    | 1078/1875 [00:26<00:20, 38.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.581:  58%|█████▊    | 1082/1875 [00:26<00:20, 37.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.581:  58%|█████▊    | 1087/1875 [00:26<00:20, 39.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  58%|█████▊    | 1087/1875 [00:26<00:20, 39.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  58%|█████▊    | 1091/1875 [00:26<00:21, 35.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  58%|█████▊    | 1095/1875 [00:27<00:21, 36.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.673:  59%|█████▊    | 1099/1875 [00:27<00:23, 33.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.722:  59%|█████▊    | 1099/1875 [00:27<00:23, 33.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.722:  59%|█████▉    | 1103/1875 [00:27<00:23, 32.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.722:  59%|█████▉    | 1107/1875 [00:27<00:22, 34.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.653:  59%|█████▉    | 1107/1875 [00:27<00:22, 34.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.653:  59%|█████▉    | 1111/1875 [00:27<00:21, 35.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.653:  60%|█████▉    | 1116/1875 [00:27<00:20, 36.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  60%|█████▉    | 1116/1875 [00:27<00:20, 36.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  60%|█████▉    | 1120/1875 [00:27<00:20, 37.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  60%|█████▉    | 1124/1875 [00:27<00:21, 35.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  60%|██████    | 1128/1875 [00:28<00:22, 33.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.670:  60%|██████    | 1128/1875 [00:28<00:22, 33.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.670:  60%|██████    | 1132/1875 [00:28<00:24, 30.87it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.670:  61%|██████    | 1136/1875 [00:28<00:22, 32.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.698:  61%|██████    | 1136/1875 [00:28<00:22, 32.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.698:  61%|██████    | 1140/1875 [00:28<00:21, 33.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.698:  61%|██████    | 1144/1875 [00:28<00:20, 34.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.698:  61%|██████    | 1148/1875 [00:28<00:21, 34.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.709:  61%|██████    | 1148/1875 [00:28<00:21, 34.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.709:  61%|██████▏   | 1152/1875 [00:28<00:21, 33.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.709:  62%|██████▏   | 1156/1875 [00:28<00:20, 35.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.747:  62%|██████▏   | 1156/1875 [00:28<00:20, 35.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.747:  62%|██████▏   | 1160/1875 [00:28<00:19, 36.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.747:  62%|██████▏   | 1164/1875 [00:29<00:19, 36.69it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.747:  62%|██████▏   | 1168/1875 [00:29<00:18, 37.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.745:  62%|██████▏   | 1168/1875 [00:29<00:18, 37.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.745:  63%|██████▎   | 1172/1875 [00:29<00:20, 35.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.745:  63%|██████▎   | 1176/1875 [00:29<00:20, 34.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.801:  63%|██████▎   | 1176/1875 [00:29<00:20, 34.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.801:  63%|██████▎   | 1180/1875 [00:29<00:20, 33.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.801:  63%|██████▎   | 1184/1875 [00:29<00:20, 34.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.801:  63%|██████▎   | 1188/1875 [00:29<00:20, 33.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.592:  63%|██████▎   | 1188/1875 [00:29<00:20, 33.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.592:  64%|██████▎   | 1192/1875 [00:29<00:19, 34.24it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.592:  64%|██████▍   | 1197/1875 [00:30<00:18, 36.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.788:  64%|██████▍   | 1197/1875 [00:30<00:18, 36.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.788:  64%|██████▍   | 1201/1875 [00:30<00:19, 34.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.788:  64%|██████▍   | 1205/1875 [00:30<00:19, 33.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.687:  64%|██████▍   | 1205/1875 [00:30<00:19, 33.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.687:  65%|██████▍   | 1210/1875 [00:30<00:18, 35.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.687:  65%|██████▍   | 1214/1875 [00:30<00:18, 35.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.687:  65%|██████▍   | 1218/1875 [00:30<00:17, 36.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  65%|██████▍   | 1218/1875 [00:30<00:17, 36.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  65%|██████▌   | 1222/1875 [00:30<00:17, 36.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  65%|██████▌   | 1226/1875 [00:30<00:17, 36.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  65%|██████▌   | 1226/1875 [00:30<00:17, 36.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  66%|██████▌   | 1231/1875 [00:30<00:16, 37.92it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  66%|██████▌   | 1235/1875 [00:31<00:17, 36.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.769:  66%|██████▌   | 1239/1875 [00:31<00:17, 36.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.683:  66%|██████▌   | 1239/1875 [00:31<00:17, 36.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.683:  66%|██████▋   | 1243/1875 [00:31<00:17, 36.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.683:  67%|██████▋   | 1248/1875 [00:31<00:16, 37.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.723:  67%|██████▋   | 1248/1875 [00:31<00:16, 37.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.723:  67%|██████▋   | 1252/1875 [00:31<00:17, 35.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.723:  67%|██████▋   | 1256/1875 [00:31<00:17, 35.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.851:  67%|██████▋   | 1256/1875 [00:31<00:17, 35.31it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.851:  67%|██████▋   | 1260/1875 [00:31<00:17, 35.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.851:  67%|██████▋   | 1264/1875 [00:31<00:17, 35.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.851:  68%|██████▊   | 1268/1875 [00:31<00:16, 36.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  68%|██████▊   | 1268/1875 [00:32<00:16, 36.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  68%|██████▊   | 1272/1875 [00:32<00:16, 36.10it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.773:  68%|██████▊   | 1276/1875 [00:32<00:16, 36.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  68%|██████▊   | 1276/1875 [00:32<00:16, 36.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  68%|██████▊   | 1280/1875 [00:32<00:16, 36.40it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  68%|██████▊   | 1284/1875 [00:32<00:16, 34.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.733:  69%|██████▊   | 1288/1875 [00:32<00:16, 35.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  69%|██████▊   | 1288/1875 [00:32<00:16, 35.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  69%|██████▉   | 1292/1875 [00:32<00:16, 35.07it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.826:  69%|██████▉   | 1296/1875 [00:32<00:16, 35.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.746:  69%|██████▉   | 1296/1875 [00:32<00:16, 35.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Training Loss: 0.746:  69%|██████▉   | 1300/1875 [00:32<00:16, 35.07it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-591bcdf76fdc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_per_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Projects/pytorch-tutorial/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train, val, epochs, batch_size, log_per_batches, learning_rate, device)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# backpropogate the loss and update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# update optimizer paraeters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_per_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# print every `log_per_batches` batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/torch/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(train=mnist, val=None, epochs=10, batch_size=32, log_per_batches=10, learning_rate=0.001, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
